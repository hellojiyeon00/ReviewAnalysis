{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e82dfdac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../tokenizer\\\\vocab.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. WordPiece 학습 및 vocab(단어 사전) 생성\n",
    "\n",
    "import os\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "# BertWordPieceTokenizer() : WordPiece 알고리즘을 사용하여 문장을 최소 의미 단위로 나누는 규칙을 학습하고 vocab(단어 사전 생성)\n",
    "# lowercase=False : 대소문자 구분\n",
    "tokenizer = BertWordPieceTokenizer(lowercase=False)\n",
    "\n",
    "# 학습에 사용할 텍스트 파일 경로\n",
    "file_path = \"../data/processed/model/pretraining_preprocessed.txt\"\n",
    "\n",
    "# 데이터셋을 학습하여 vocab(단어 사전) 추출\n",
    "tokenizer.train(\n",
    "    files=file_path,\n",
    "    vocab_size=30000,       # vocab(단어 사전) 크기\n",
    "    limit_alphabet=3000,    # 개별 문자/특수 문자 등의 최대 개수 제한\n",
    "    min_frequency=5,        # 최소 등장 빈도\n",
    ")\n",
    "\n",
    "# 파일을 저장할 폴더 경로\n",
    "dir_path = \"../tokenizer\"\n",
    "\n",
    "# 해당 폴더가 없으면\n",
    "if not os.path.exists(dir_path):\n",
    "    # 폴더 생성\n",
    "    os.mkdir(dir_path)\n",
    "\n",
    "# vocab(단어 사전) 저장\n",
    "tokenizer.save_model(dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b6d2381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. tokenizer 초기화\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# 학습된 vocab(단어 사전) 경로\n",
    "vocab_file_path = \"../tokenizer/vocab.txt\"\n",
    "\n",
    "# BertTokenizer() : 학습된 vocab(단어 사전)을 기반으로 문장을 토큰화하고 정수 인덱싱을 할 수 있는 객체로 초기화\n",
    "tokenizer = BertTokenizer(\n",
    "    vocab_file=vocab_file_path,\n",
    "    do_lower_case=False,  # 한국어는 대소문자 구분 필요 없음\n",
    "    unk_token=\"[UNK]\",\n",
    "    sep_token=\"[SEP]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    cls_token=\"[CLS]\",\n",
    "    mask_token=\"[MASK]\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44b9a0bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../tokenizer\\\\tokenizer_config.json',\n",
       " '../tokenizer\\\\special_tokens_map.json',\n",
       " '../tokenizer\\\\vocab.txt',\n",
       " '../tokenizer\\\\added_tokens.json')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 토크나이저를 저장할 경로\n",
    "dir_path = \"../tokenizer\"\n",
    "\n",
    "# save_pretrained() : 저장\n",
    "tokenizer.save_pretrained(dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06e8a00b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [2, 11102, 4858, 13, 4269, 3919, 4926, 13, 1546, 2526, 1937, 4552, 3421, 5415, 13, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "3919\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"../tokenizer\")\n",
    "# result = tokenizer.tokenize(\"안녕하세요. 저는 학생 입니다. 코딩을 공부하고 있어요.\")\n",
    "result = tokenizer(\"안녕하세요. 저는 학생 입니다. 코딩을 공부하고 있어요.\")\n",
    "print(result)\n",
    "    \n",
    "print(tokenizer.vocab['학생'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273d5948",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Re_BERT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
