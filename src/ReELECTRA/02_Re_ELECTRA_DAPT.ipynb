{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ak8LgoWHCyCt"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.optim import AdamW\n",
    "from transformers import Trainer, TrainingArguments, TrainerCallback, TrainerState, TrainerControl, AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "\n",
    "# ElectraTrainer 정의\n",
    "class ElectraTrainer(Trainer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        generator,\n",
    "        discriminator,\n",
    "        gen_optimizer,\n",
    "        disc_optimizer,\n",
    "        processing_class,\n",
    "        warmup_steps=0,\n",
    "        *args,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(model=discriminator, optimizers=(gen_optimizer, None), *args, **kwargs)\n",
    "        self.generator = generator\n",
    "        self.discriminator = discriminator\n",
    "        self.gen_optimizer = gen_optimizer\n",
    "        self.disc_optimizer = disc_optimizer\n",
    "        self.processing_class = processing_class\n",
    "        self.bce_loss_fn = BCEWithLogitsLoss()\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.global_step_counter = 0\n",
    "\n",
    "    def training_step(self, model, inputs, loss_or_steps=None, **kwargs):\n",
    "        inputs = self._prepare_inputs(inputs)\n",
    "        generator = self.generator\n",
    "        discriminator = self.discriminator\n",
    "        attention_mask = inputs[\"attention_mask\"]\n",
    "        real_input_ids = inputs[\"input_ids\"]\n",
    "        labels = inputs[\"labels\"]\n",
    "\n",
    "        self.global_step_counter += 1\n",
    "\n",
    "        # Generator 학습 (MLM)\n",
    "        self.gen_optimizer.zero_grad()\n",
    "        gen_outputs = generator(input_ids=real_input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        gen_loss = gen_outputs.loss\n",
    "\n",
    "        if torch.isnan(gen_loss) or torch.isinf(gen_loss):\n",
    "            # print(f\"Step {self.global_step_counter} | Generator Loss NaN/Inf → skip\")\n",
    "            return torch.tensor(0.0, device=gen_loss.device)\n",
    "\n",
    "        self.accelerator.backward(gen_loss, retain_graph=True)\n",
    "\n",
    "        # Fake 문장 생성\n",
    "        with torch.no_grad():\n",
    "            gen_predictions = gen_outputs.logits.argmax(dim=-1)\n",
    "            fake_inputs = real_input_ids.clone()\n",
    "            mask = real_input_ids == self.processing_class.mask_token_id\n",
    "            fake_inputs[mask] = gen_predictions[mask]\n",
    "\n",
    "        # Discriminator 학습 (RTD)\n",
    "        if self.global_step_counter > self.warmup_steps:\n",
    "            self.disc_optimizer.zero_grad()\n",
    "            disc_labels = (real_input_ids != fake_inputs).float()\n",
    "            disc_outputs = discriminator(input_ids=fake_inputs, attention_mask=attention_mask)\n",
    "            disc_logits = disc_outputs.logits.squeeze(-1)\n",
    "            disc_loss = self.bce_loss_fn(disc_logits, disc_labels)\n",
    "\n",
    "            if torch.isnan(disc_loss) or torch.isinf(disc_loss):\n",
    "                # print(f\"Step {self.global_step_counter} | Discriminator Loss NaN/Inf → skip\")\n",
    "                # loss는 0.0으로 로깅을 위해 설정하지만, backward는 호출하지 않음\n",
    "                valid_disc_loss = False\n",
    "                log_disc_loss = torch.tensor(0.0, device=gen_loss.device)\n",
    "            else:\n",
    "                valid_disc_loss = True\n",
    "                log_disc_loss = disc_loss # 로깅을 위해 실제 loss 사용\n",
    "\n",
    "            if valid_disc_loss:\n",
    "                self.accelerator.backward(disc_loss)\n",
    "\n",
    "            disc_loss = log_disc_loss # 이후 로깅 및 total_loss 계산에 사용\n",
    "\n",
    "        else:\n",
    "            # 웜업 단계에서는 backward 호출 안 함\n",
    "            disc_loss = torch.tensor(0.0, device=gen_loss.device)\n",
    "            log_disc_loss = disc_loss\n",
    "            valid_disc_loss = False\n",
    "\n",
    "        # Gradient Clipping\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            list(generator.parameters()) + list(discriminator.parameters()),\n",
    "            max_norm=self.args.max_grad_norm\n",
    "        )\n",
    "\n",
    "        # Optimizer Step\n",
    "        self.gen_optimizer.step()\n",
    "        if self.global_step_counter > self.warmup_steps:\n",
    "            self.disc_optimizer.step()\n",
    "\n",
    "        # Scheduler Step\n",
    "        if self.lr_scheduler is not None:\n",
    "            self.lr_scheduler.step()\n",
    "\n",
    "        # Total Loss 반환 및 로깅\n",
    "        total_loss = gen_loss + disc_loss\n",
    "        self.log({\n",
    "            \"gen_loss\": gen_loss.detach().item(),\n",
    "            \"disc_loss\": disc_loss.detach().item(),\n",
    "            \"total_loss\": total_loss.detach().item()\n",
    "        })\n",
    "        return total_loss.detach()\n",
    "\n",
    "\n",
    "# 커스텀 로깅 콜백\n",
    "class ElectraLoggingCallback(TrainerCallback):\n",
    "    def on_log(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, logs=None, **kwargs):\n",
    "        if logs is not None and 'loss' in logs:\n",
    "            last_log = state.log_history[-1]\n",
    "            if 'gen_loss' in last_log and 'disc_loss' in last_log:\n",
    "                current_step = last_log.get('step', state.global_step)\n",
    "                if current_step % args.logging_steps == 0:\n",
    "                    print(f\"Step {current_step} | Generator Loss: {last_log['gen_loss']:.4f} | Discriminator Loss: {last_log['disc_loss']:.4f}\")\n",
    "\n",
    "    def on_step_end(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n",
    "        # 1000 스텝마다 체크포인트 저장 플래그를 True로 설정\n",
    "        if state.global_step % 1000 == 0 and state.global_step > 0:\n",
    "            control.should_save = True\n",
    "\n",
    "\n",
    "# 커스텀 체크포인트 콜백 (Generator + Discriminator 동시 저장)\n",
    "class ElectraCheckpointCallback(TrainerCallback):\n",
    "    def on_save(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n",
    "        if isinstance(trainer, ElectraTrainer):\n",
    "            # 체크포인트 폴더 생성\n",
    "            output_dir = os.path.join(args.output_dir, f\"checkpoint-{state.global_step}\")\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "            # Generator 저장\n",
    "            torch.save(trainer.generator.state_dict(), os.path.join(output_dir, \"generator.pt\"))\n",
    "\n",
    "            # Discriminator 저장\n",
    "            torch.save(trainer.discriminator.state_dict(), os.path.join(output_dir, \"discriminator.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "path = str(pathlib.Path(os.path.abspath(\".\")).parent.parent)\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2868,
     "status": "ok",
     "timestamp": 1762571446484,
     "user": {
      "displayName": "jiyeon",
      "userId": "06315727781370043567"
     },
     "user_tz": -540
    },
    "id": "cbur1PQYC0ds",
    "outputId": "40c2468f-5de5-4a46-d25a-cee6aaa7d26e"
   },
   "outputs": [],
   "source": [
    "from transformers import ElectraForMaskedLM, ElectraForPreTraining, AutoTokenizer\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# Generator & Discriminator 초기화\n",
    "generator = ElectraForMaskedLM.from_pretrained(\"google/electra-small-generator\")\n",
    "discriminator = ElectraForPreTraining.from_pretrained(\"google/electra-small-discriminator\")\n",
    "\n",
    "# 한국어 토크나이저 사용\n",
    "tokenizer = AutoTokenizer.from_pretrained(f\"{path}/tokenizer\")\n",
    "\n",
    "# Optimizer 설정 (Generator / Discriminator 분리)\n",
    "discriminator_lr = 5e-5\n",
    "generator_lr = discriminator_lr * 0.5  # Generator는 더 작은 lr 사용\n",
    "\n",
    "gen_optimizer = AdamW(generator.parameters(), lr=generator_lr)\n",
    "disc_optimizer = AdamW(discriminator.parameters(), lr=discriminator_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1702,
     "status": "ok",
     "timestamp": 1762571448220,
     "user": {
      "displayName": "jiyeon",
      "userId": "06315727781370043567"
     },
     "user_tz": -540
    },
    "id": "_iriROS7IvIl",
    "outputId": "2bf54f38-9005-4343-830e-0a67aebe22e5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# 마지막에 저장된 checkpoint 숫자\n",
    "# Re_ELECTRA_checkpoint -> checkpoint-{step}\n",
    "step = 135000\n",
    "last_checkpoint_path = f\"{path}/model/ReELECTRA/pretrained/checkpoints/checkpoint-{step}\"\n",
    "\n",
    "print(f\"체크포인트 {last_checkpoint_path} 에서 상태를 로드합니다.\")\n",
    "\n",
    "try:\n",
    "    # Generator 및 Discriminator 상태 로드\n",
    "    gen_checkpoint_path = os.path.join(last_checkpoint_path, \"generator.pt\")\n",
    "    generator.load_state_dict(torch.load(gen_checkpoint_path))\n",
    "    print(f\"Loaded Generator from {gen_checkpoint_path}\")\n",
    "\n",
    "    disc_checkpoint_path = os.path.join(last_checkpoint_path, \"discriminator.pt\")\n",
    "    discriminator.load_state_dict(torch.load(disc_checkpoint_path))\n",
    "    print(f\"Loaded Discriminator from {disc_checkpoint_path}\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"체크포인트 파일 로드 실패: {e}\")\n",
    "    print(\"→ 초기 모델부터 DAPT 학습을 시작합니다.\")\n",
    "    last_checkpoint_path = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "29a742c2076e4f7bb6458297f919b82f",
      "c4724814626b4d0f86406ae69e1dddbf",
      "e6fc51a197a947efa40755a57894a6d9",
      "808700e084734bc1874f4651ae40b355",
      "93b4b597ff0a425e99abc6be909b8689",
      "4f93abecb8b34fc69b0b51a819f85029",
      "efb0a0a8834d4765b161bb9da668b1fa",
      "f0f202856b294b42889ecdf97d6ad24e",
      "727725a166c4481a83577b897ce5485f",
      "94efff573faf4c8d8d0a9392f7c8ffc7",
      "ccfc4ee222db48f78e65e54f51bd8288"
     ]
    },
    "executionInfo": {
     "elapsed": 3908,
     "status": "ok",
     "timestamp": 1762571452132,
     "user": {
      "displayName": "jiyeon",
      "userId": "06315727781370043567"
     },
     "user_tz": -540
    },
    "id": "YCVU5Q4kC1ao",
    "outputId": "d1bbc39b-60e6-40d1-f8be-f7adf468c09a"
   },
   "outputs": [],
   "source": [
    "# 데이터셋 객체로 변환\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# 텍스트 파일 경로\n",
    "file_path = f\"{path}/data/processed/model/dapt_preprocessed.txt\"\n",
    "\n",
    "datasets = load_dataset(\"text\", data_files=file_path)\n",
    "\n",
    "# print(datasets.keys())\n",
    "\n",
    "# train 데이터에서 상위 5000개만 선택\n",
    "# datasets['train'] = datasets['train'].select(range(5000))\n",
    "\n",
    "# type : DatasetDict, Dataset 객체를 dict 형태로 묶어 관리\n",
    "# print(type(datasets))\n",
    "# 전체 key 값 확인\n",
    "# print(datasets.keys())\n",
    "# 해당 key 값의 인덱스 10에 해당하는 값 출력\n",
    "# print(datasets['train'][10])\n",
    "\n",
    "# tokenize_function() : 텍스트 토큰화 함수 정의\n",
    "def tokenize_function(examples):\n",
    "    from transformers import AutoTokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(f\"{path}/tokenizer\")\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "    )\n",
    "\n",
    "# datasets 토큰화\n",
    "tokenized_datasets = datasets.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    num_proc=2, # 프로세스 개수\n",
    "    remove_columns=[\"text\"],\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=True,\n",
    "    mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ME5XwRYC3h8"
   },
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "# train 80%, validation 20% 분할\n",
    "split_dataset = tokenized_datasets[\"train\"].train_test_split(test_size=0.2)\n",
    "tokenized_datasets = DatasetDict({\n",
    "    \"train\": split_dataset[\"train\"],\n",
    "    \"validation\": split_dataset[\"test\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 109,
     "status": "ok",
     "timestamp": 1762571452340,
     "user": {
      "displayName": "jiyeon",
      "userId": "06315727781370043567"
     },
     "user_tz": -540
    },
    "id": "OSox9mVJC5gR",
    "outputId": "ef2cc126-f9ff-4076-dc2e-30e9f2d48a94"
   },
   "outputs": [],
   "source": [
    "# TrainingArguments\n",
    "\n",
    "import torch\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if device==\"cpu\":\n",
    "    num_train_epochs=1\n",
    "    per_device_train_batch_size=2\n",
    "    gradient_accumulation_steps=8\n",
    "    dataloader_num_workers=0\n",
    "\n",
    "elif device==\"cuda\":\n",
    "    num_train_epochs=2\n",
    "    per_device_train_batch_size=16\n",
    "    gradient_accumulation_steps=1\n",
    "    dataloader_num_workers=2\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"{path}/model/ReELECTRA/DAPT/checkpoints\",      # 체크포인트, 로그가 저장될 경로\n",
    "    overwrite_output_dir=True,                                  # 기존 경로 덮어쓰기(허용)\n",
    "\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "\n",
    "    max_grad_norm=1.0,\n",
    "\n",
    "    logging_steps=50,\n",
    "\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1000,\n",
    "\n",
    "    dataloader_num_workers=dataloader_num_workers,\n",
    "\n",
    "    report_to=\"none\",\n",
    "    push_to_hub=False,\n",
    "    hub_model_id=None,\n",
    "    hub_token=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1MtXTAmbC7NA"
   },
   "outputs": [],
   "source": [
    "generator.to(device)\n",
    "discriminator.to(device)\n",
    "\n",
    "# Trainer 설정\n",
    "\n",
    "trainer = ElectraTrainer(\n",
    "    generator=generator,\n",
    "    discriminator=discriminator,\n",
    "    gen_optimizer=gen_optimizer,\n",
    "    disc_optimizer=disc_optimizer,\n",
    "    processing_class=tokenizer,\n",
    "    warmup_steps=1000,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[ElectraLoggingCallback(), ElectraCheckpointCallback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 426
    },
    "executionInfo": {
     "elapsed": 87400,
     "status": "ok",
     "timestamp": 1762571540082,
     "user": {
      "displayName": "jiyeon",
      "userId": "06315727781370043567"
     },
     "user_tz": -540
    },
    "id": "Kl5OrqZZC85U",
    "outputId": "c3ce0094-41a7-421e-d630-0586fe5db5e4"
   },
   "outputs": [],
   "source": [
    "# 모델 학습\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9FJ7xqGTDAQ4"
   },
   "outputs": [],
   "source": [
    "# 모델 저장\n",
    "\n",
    "trainer.save_model(f\"{path}/model/ReELECTRA/DAPT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6963,
     "status": "ok",
     "timestamp": 1762571637114,
     "user": {
      "displayName": "jiyeon",
      "userId": "06315727781370043567"
     },
     "user_tz": -540
    },
    "id": "3Jv-RrWydK6e",
    "outputId": "f5be1612-00c1-45ef-bcf0-483b43599770"
   },
   "outputs": [],
   "source": [
    "# 모델 평가\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "# 평가용 DataLoader\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"validation\"],\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "generator.eval()\n",
    "discriminator.eval()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "gen_correct, gen_total = 0, 0\n",
    "disc_correct, disc_total = 0, 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(eval_dataloader, desc=\"평가 진행 중\"):\n",
    "        # GPU로 이동\n",
    "        inputs = {k: v.to(device) for k, v in batch.items() if k in [\"input_ids\", \"attention_mask\", \"labels\"]}\n",
    "\n",
    "        # Generator 평가 (MLM 정확도)\n",
    "        gen_outputs = generator(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            labels=inputs[\"labels\"]\n",
    "        )\n",
    "        gen_logits = gen_outputs.logits\n",
    "        mask = inputs[\"labels\"] != -100  # -100은 무시 토큰\n",
    "        gen_preds = gen_logits.argmax(dim=-1)\n",
    "\n",
    "        gen_correct += (gen_preds[mask] == inputs[\"labels\"][mask]).sum().item()\n",
    "        gen_total += mask.sum().item()\n",
    "\n",
    "        # Discriminator 평가 (RTD 정확도)\n",
    "        # Generator 예측으로 fake 문장 생성\n",
    "        gen_predictions = gen_logits.argmax(dim=-1)\n",
    "        fake_inputs = inputs[\"input_ids\"].clone()\n",
    "        fake_inputs[inputs[\"labels\"] != -100] = gen_predictions[inputs[\"labels\"] != -100]\n",
    "\n",
    "        # Fake token label 생성\n",
    "        disc_labels = (inputs[\"input_ids\"] != fake_inputs).float()\n",
    "\n",
    "        disc_outputs = discriminator(input_ids=fake_inputs, attention_mask=inputs[\"attention_mask\"])\n",
    "        disc_logits = torch.sigmoid(disc_outputs.logits.squeeze(-1))\n",
    "        disc_preds = (disc_logits > 0.5).float()\n",
    "\n",
    "        disc_correct += (disc_preds == disc_labels).sum().item()\n",
    "        disc_total += disc_labels.numel()\n",
    "\n",
    "# 정확도 계산\n",
    "gen_accuracy = gen_correct / gen_total\n",
    "disc_accuracy = disc_correct / disc_total\n",
    "\n",
    "print(f\"\\n✅ Generator MLM 정확도: {gen_accuracy*100:.2f}%\")\n",
    "print(f\"✅ Discriminator RTD 정확도: {disc_accuracy*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 133,
     "status": "ok",
     "timestamp": 1762571784982,
     "user": {
      "displayName": "jiyeon",
      "userId": "06315727781370043567"
     },
     "user_tz": -540
    },
    "id": "CdVDwcnLdLV5",
    "outputId": "0a2afbd4-57f1-4c52-cb5d-8bcad39da567"
   },
   "outputs": [],
   "source": [
    "masked_texts = [\n",
    "    \"[MASK]이 느려요.\"\n",
    "]\n",
    "\n",
    "for text in masked_texts:\n",
    "\n",
    "    # 토큰화\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Generator 예측\n",
    "    generator.eval()\n",
    "    with torch.no_grad():\n",
    "        gen_outputs = generator(**inputs)\n",
    "        gen_logits = gen_outputs.logits\n",
    "        mask_token_index = (inputs[\"input_ids\"] == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
    "        pred_token_id = gen_logits[0, mask_token_index].argmax(dim=-1)\n",
    "        pred_token = tokenizer.decode(pred_token_id)\n",
    "\n",
    "    # Discriminator 판단\n",
    "    discriminator.eval()\n",
    "    fake_inputs = inputs[\"input_ids\"].clone()\n",
    "    fake_inputs[0, mask_token_index] = pred_token_id\n",
    "\n",
    "    disc_outputs = discriminator(input_ids=fake_inputs, attention_mask=inputs[\"attention_mask\"])\n",
    "    disc_logits = torch.sigmoid(disc_outputs.logits.squeeze(-1))\n",
    "    is_fake = disc_logits[0, mask_token_index] > 0.5\n",
    "\n",
    "    # 결과 출력\n",
    "    print(f\"\\n원문: {text}\")\n",
    "    print(f\"Generator 예측: {pred_token}\")\n",
    "    print(f\"Discriminator 판단: {'FAKE' if is_fake else 'REAL'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zkXHKyhcDFzs"
   },
   "source": [
    "checkpoint를 사용하여 중간부터 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zHYMONxiDHv5"
   },
   "outputs": [],
   "source": [
    "from transformers import ElectraForMaskedLM, ElectraForPreTraining, AutoTokenizer\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# Generator & Discriminator 초기화\n",
    "generator = ElectraForMaskedLM.from_pretrained(\"google/electra-small-generator\")\n",
    "discriminator = ElectraForPreTraining.from_pretrained(\"google/electra-small-discriminator\")\n",
    "\n",
    "# 한국어 토크나이저 사용\n",
    "tokenizer = AutoTokenizer.from_pretrained(f\"{path}/tokenizer\")\n",
    "\n",
    "# Optimizer 설정 (Generator / Discriminator 분리)\n",
    "discriminator_lr = 5e-5\n",
    "generator_lr = discriminator_lr * 0.5  # Generator는 더 작은 lr 사용\n",
    "\n",
    "gen_optimizer = AdamW(generator.parameters(), lr=generator_lr)\n",
    "disc_optimizer = AdamW(discriminator.parameters(), lr=discriminator_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1202,
     "status": "ok",
     "timestamp": 1762571866960,
     "user": {
      "displayName": "jiyeon",
      "userId": "06315727781370043567"
     },
     "user_tz": -540
    },
    "id": "vYRllR0IDJoc",
    "outputId": "579344ce-56ad-4d66-c136-8434c6795415"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# 마지막에 저장된 checkpoint 숫자\n",
    "# Re_ELECTRA_checkpoint -> checkpoint-{step}\n",
    "step = \n",
    "last_checkpoint_path = f\"{path}/model/ReELECTRA/DAPT/checkpoints/checkpoint-{step}\"\n",
    "\n",
    "print(f\"체크포인트 {last_checkpoint_path} 에서 상태를 로드합니다.\")\n",
    "\n",
    "try:\n",
    "    # Generator 및 Discriminator 상태 로드\n",
    "    gen_checkpoint_path = os.path.join(last_checkpoint_path, \"generator.pt\")\n",
    "    generator.load_state_dict(torch.load(gen_checkpoint_path))\n",
    "    print(f\"Loaded Generator from {gen_checkpoint_path}\")\n",
    "\n",
    "    disc_checkpoint_path = os.path.join(last_checkpoint_path, \"discriminator.pt\")\n",
    "    discriminator.load_state_dict(torch.load(disc_checkpoint_path))\n",
    "    print(f\"Loaded Discriminator from {disc_checkpoint_path}\")\n",
    "\n",
    "    optim_checkpoint_path = os.path.join(last_checkpoint_path, \"optimizer.pt\")\n",
    "\n",
    "    if os.path.exists(optim_checkpoint_path):\n",
    "        optim_checkpoint = torch.load(optim_checkpoint_path)\n",
    "        print(f\"Found optimizer state at {optim_checkpoint_path}. Trainer will handle full restore.\")\n",
    "\n",
    "    else:\n",
    "        # 옵티마이저 파일이 없으면 Trainer가 초기화된 옵티마이저로 시작할 수 있습니다.\n",
    "        print(\"optimizer.pt 파일이 없습니다. 옵티마이저 상태가 초기화될 수 있습니다.\")\n",
    "\n",
    "    # 모델 로드가 성공했으므로, 재개를 위해 last_checkpoint_path를 유지\n",
    "    last_checkpoint_path = last_checkpoint_path\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"체크포인트 파일 로드 실패: {e}\")\n",
    "    print(\"→ 초기 모델부터 DAPT 학습을 시작합니다.\")\n",
    "    last_checkpoint_path = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tlfK4EX-DNCQ"
   },
   "outputs": [],
   "source": [
    "if device == \"cuda\":\n",
    "    generator.to(device)\n",
    "    discriminator.to(device)\n",
    "\n",
    "# Trainer 설정\n",
    "\n",
    "trainer = ElectraTrainer(\n",
    "    generator=generator,\n",
    "    discriminator=discriminator,\n",
    "    gen_optimizer=gen_optimizer,\n",
    "    disc_optimizer=disc_optimizer,\n",
    "    processing_class=tokenizer,\n",
    "    warmup_steps=1000,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[ElectraLoggingCallback(), ElectraCheckpointCallback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 426
    },
    "executionInfo": {
     "elapsed": 51976,
     "status": "ok",
     "timestamp": 1762571594297,
     "user": {
      "displayName": "jiyeon",
      "userId": "06315727781370043567"
     },
     "user_tz": -540
    },
    "id": "fcbjOlh4DNpD",
    "outputId": "479d57a0-33b1-45e2-e319-cead63377480"
   },
   "outputs": [],
   "source": [
    "# 학습 재개\n",
    "\n",
    "trainer.train(resume_from_checkpoint=last_checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "znPadwAUDQJA"
   },
   "outputs": [],
   "source": [
    "# 모델 저장\n",
    "\n",
    "trainer.save_model(f\"{path}/model/ReELECTRA/DAPT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pOb3NnlRdTOM"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMayg6JBdkWYK08h70tnZYB",
   "gpuType": "T4",
   "mount_file_id": "14Ckn-7mnmH0627wuP_4c8LDtZYCukG3n",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "29a742c2076e4f7bb6458297f919b82f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c4724814626b4d0f86406ae69e1dddbf",
       "IPY_MODEL_e6fc51a197a947efa40755a57894a6d9",
       "IPY_MODEL_808700e084734bc1874f4651ae40b355"
      ],
      "layout": "IPY_MODEL_93b4b597ff0a425e99abc6be909b8689"
     }
    },
    "4f93abecb8b34fc69b0b51a819f85029": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "727725a166c4481a83577b897ce5485f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "808700e084734bc1874f4651ae40b355": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_94efff573faf4c8d8d0a9392f7c8ffc7",
      "placeholder": "​",
      "style": "IPY_MODEL_ccfc4ee222db48f78e65e54f51bd8288",
      "value": " 5000/5000 [00:03&lt;00:00, 1488.73 examples/s]"
     }
    },
    "93b4b597ff0a425e99abc6be909b8689": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "94efff573faf4c8d8d0a9392f7c8ffc7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c4724814626b4d0f86406ae69e1dddbf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4f93abecb8b34fc69b0b51a819f85029",
      "placeholder": "​",
      "style": "IPY_MODEL_efb0a0a8834d4765b161bb9da668b1fa",
      "value": "Map (num_proc=2): 100%"
     }
    },
    "ccfc4ee222db48f78e65e54f51bd8288": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e6fc51a197a947efa40755a57894a6d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f0f202856b294b42889ecdf97d6ad24e",
      "max": 5000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_727725a166c4481a83577b897ce5485f",
      "value": 5000
     }
    },
    "efb0a0a8834d4765b161bb9da668b1fa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f0f202856b294b42889ecdf97d6ad24e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
